import joblib
import torch
import jieba
import warnings
import re
from torch.utils.data import Dataset
from tqdm import tqdm

from util import logger, root_dir, args
from pytorch_transformers import BertTokenizer
from config import *

# ===================== å…¨å±€é…ç½® =====================
warnings.filterwarnings('ignore')
import nltk
import ssl

ssl._create_default_https_context = ssl._create_unverified_context
try:
    nltk.data.find('omw-1.4')
except LookupError:
    nltk.download('omw-1.4', quiet=True)
from nltk.corpus import wordnet as wn
from collections import Counter

# âœ… å…³é”®é…ç½®ï¼šå–æ¶ˆæˆªæ–­+æ–‡æœ¬æ¸…æ´—
NO_MAX_LEN = True  # å…³é—­é•¿åº¦é™åˆ¶ï¼ŒåŠ¨æ€é€‚é…æ–‡æœ¬
CLEAN_PATTERN = r'(left:|right:|\n|\s+|:)'  # æ¸…æ´—æ ¼å¼ç¬¦
# âœ… æ ‡ç‚¹ç¬¦å·é»‘åå•ï¼ˆå¼ºåˆ¶æ ‡è®°ä¸º0ï¼‰
PUNCTUATIONS = {"ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", "ï¼š", "ï¼›", "ã€", "â€œ", "â€", "ï¼ˆ", "ï¼‰", "ã€Š", "ã€‹", ".", "?", "!", ";", ":"}


# ===================== æ•°æ®é›†ç±»ï¼ˆç²¾å‡†ä¿®å¤ç‰ˆï¼‰ =====================
class FraudAttackDataset(Dataset):
    def __init__(self, path):
        cache_path = 'FT_FC_' + path
        save_path = 'all_' + path
        self.data = joblib.load(cache_path)
        knowledge_data = []
        self.tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
        self.word_list = joblib.load(args.word_list)
        self.word_list_set = set(self.word_list)

        print("=" * 80)
        print("å¯åŠ¨ã€ç²¾å‡†ä¿®å¤ç‰ˆã€‘æ•°æ®å¤„ç† | ä¿®å¤start_markæ ‡è®°é”™ä½ | å…±{}æ¡æ ·æœ¬".format(len(self.data)))
        print("=" * 80)

        for i, data in enumerate(tqdm(self.data, desc="æ•°æ®å¤„ç†è¿›åº¦", unit="æ¡")):
            data['knowledge_dict'] = {}
            data['start_mark'] = []
            data['seq'] = []
            data['seq_len'] = 0

            try:
                if 'raw_text' in data and data['raw_text'].strip():
                    raw_text = data['raw_text'].strip()
                    clean_text = self.clean_raw_text(raw_text)  # æ¸…æ´—æ–‡æœ¬
                    # âœ… ç”Ÿæˆã€æ— æˆªæ–­seq + ç²¾å‡†start_mark + çº¯å‡€è¯å…¸ã€‘
                    knowledge_dict, bert_seq, seq_len, start_mark = self.process_raw_text(clean_text)
                    data['knowledge_dict'] = knowledge_dict
                    data['seq'] = bert_seq
                    data['seq_len'] = seq_len
                    data['start_mark'] = start_mark
                    # æ¸…æ´—è¯å…¸æ— æ•ˆé”®
                    if 'similar_dict' in data:
                        data['similar_dict'] = self.clean_dict(data['similar_dict'])
                else:
                    if 'seq' in data and len(data['seq']) > 0:
                        data['start_mark'] = self.gen_start_mark_perfect(data['seq'])
                        data['seq_len'] = len(data['seq'])

                if len(data['seq']) > 0 and len(data['start_mark']) > 0:
                    knowledge_data.append(data)

                if i % 500 == 0 and i > 0:
                    joblib.dump(knowledge_data, save_path)
                    print("è¿›åº¦ä¿å­˜ï¼š{}æ¡æ ·æœ¬å·²å­˜å…¥ {}".format(i, save_path))

            except Exception as e:
                print("æ ·æœ¬{}å¤„ç†å¼‚å¸¸ï¼š{} â†’ è·³è¿‡".format(i, str(e)[:50]))
                continue

        joblib.dump(knowledge_data, save_path)
        print("=" * 80)
        print("âœ… å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜è‡³ {} | æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{}".format(save_path, len(knowledge_data)))
        print("=" * 80)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return self.data[index]

    def clean_raw_text(self, raw_text):
        """æ¸…æ´—åŸå§‹æ–‡æœ¬ï¼Œä»…ä¿ç•™çº¯ä¸­æ–‡+åˆæ³•æ ‡ç‚¹"""
        clean_text = re.sub(CLEAN_PATTERN, '', raw_text)
        clean_text = re.sub(r'[^\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€]', '', clean_text)
        return clean_text.strip()

    def clean_dict(self, origin_dict):
        """æ¸…æ´—è¯å…¸ï¼Œä»…ä¿ç•™çº¯ä¸­æ–‡è¯æ±‡é”®"""
        clean_dict = {}
        for k, v in origin_dict.items():
            if k.strip() and re.match(r'^[\u4e00-\u9fa5]+$', k.strip()):
                clean_dict[k] = v
        return clean_dict

    def process_raw_text(self, clean_text):
        # ç”Ÿæˆæ— æˆªæ–­seq
        tokens = self.tokenizer.tokenize(clean_text)
        bert_tokens = ['[CLS]'] + tokens + ['[SEP]']
        bert_seq = self.tokenizer.convert_tokens_to_ids(bert_tokens)
        seq_len = len(bert_seq)

        # ç”Ÿæˆçº¯å‡€è¯å…¸
        jieba_words = [w.strip() for w in jieba.lcut(clean_text) if w.strip() and re.match(r'^[\u4e00-\u9fa5]+$', w)]
        knowledge_dict = {w: self.get_knowledge(w) if len(w) >= 2 else [w] for w in jieba_words}

        # âœ… è°ƒç”¨æ–°æ ‡è®°å‡½æ•°ï¼ˆJiebaå¤šå­—è¯ç²’åº¦ï¼‰
        start_mark = self.gen_start_mark_jieba_align(bert_seq, clean_text)

        return knowledge_dict, bert_seq, seq_len, start_mark
    def gen_start_mark_jieba_align(self, seq, clean_text):
        """
        âœ… æ ¸å¿ƒä¿®æ”¹ï¼šåŸºäºJiebaåˆ†è¯ç»“æœæ ‡è®°ï¼Œå®ç°ã€Œå¤šå­—è¯ç²’åº¦æ‹†åˆ†ã€
        :param seq: BERTçš„seqï¼ˆæ•°å­—IDåˆ—è¡¨ï¼‰
        :param clean_text: æ¸…æ´—åçš„åŸå§‹æ–‡æœ¬
        :return: start_markï¼ˆå¤šå­—è¯ç²’åº¦æ ‡è®°ï¼Œä¸Jiebaåˆ†è¯å¯¹é½ï¼‰
        """
        # æ­¥éª¤1ï¼šå¯¹åŸå§‹æ–‡æœ¬åšJiebaåˆ†è¯ï¼ˆå¾—åˆ°å¤šå­—è¯ç»“æœï¼‰
        jieba_words = jieba.lcut(clean_text)  # ä¾‹ï¼š["å–‚", "ä½ å¥½", "æ˜¯", "å¼ æ€»", "å—"]

        # æ­¥éª¤2ï¼šå°†seqè½¬ä¸ºBERT tokenåˆ—è¡¨
        token_list = [self.tokenizer._convert_id_to_token(id) for id in seq]
        start_mark = [0] * len(token_list)  # åˆå§‹åŒ–å…¨0

        # æ­¥éª¤3ï¼šè·³è¿‡ç‰¹æ®Štoken [CLS]/[SEP]
        valid_token_list = token_list[1:-1]  # å»æ‰[CLS]å’Œ[SEP]
        current_token_idx = 0  # è¿½è¸ªå½“å‰å¤„ç†åˆ°çš„BERT tokenä¸‹æ ‡

        # æ­¥éª¤4ï¼šéå†Jiebaåˆ†è¯ç»“æœï¼Œå¼ºåˆ¶æ ‡è®°å¤šå­—è¯ç²’åº¦
        for word in jieba_words:
            # è·³è¿‡æ ‡ç‚¹
            if word in PUNCTUATIONS:
                current_token_idx += len(word)
                continue

            # å¤šå­—è¯çš„é¦–å­— â†’ æ ‡1
            if current_token_idx < len(valid_token_list):
                # å¯¹åº”åˆ°åŸå§‹seqçš„ä¸‹æ ‡ï¼ˆ+1æ˜¯å› ä¸ºè·³è¿‡äº†[CLS]ï¼‰
                seq_idx = current_token_idx + 1
                start_mark[seq_idx] = 1

            # å¤šå­—è¯çš„åç»­å­— â†’ æ ‡0ï¼ˆè‡ªåŠ¨ä¿æŒ0ï¼Œæ— éœ€é¢å¤–æ“ä½œï¼‰
            current_token_idx += len(word)  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå¤šå­—è¯çš„é¦–å­—ä½ç½®

        # ç‰¹æ®Štoken [CLS]/[SEP]å¼ºåˆ¶æ ‡0
        start_mark[0] = 0
        start_mark[-1] = 0

        return start_mark

    def get_knowledge(self, word):
        knowledge = [word]
        try:
            synset = wn.synsets(word, lang='cmn')
            if synset:
                posset = [syn.name().split('.')[1] for syn in synset if '.' in syn.name()]
                if posset:
                    pos = Counter(posset).most_common(1)[0][0]
                    new_synset = [lemma for syn in synset for lemma in syn.lemma_names(lang='cmn')]
                    knowledge = list(set(new_synset + [word]))
        except:
            pass
        return knowledge


# ===================== é…å¥—å·¥å…·å‡½æ•° + è‡ªåŠ¨æ ¡éªŒå‡½æ•° =====================
def transform(seq):
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
    if not isinstance(seq, list):
        seq = seq.squeeze().cpu().numpy().tolist()
    tokens = [tokenizer._convert_id_to_token(x) for x in seq if x != 0 and tokenizer._convert_id_to_token(x)]
    return tokenizer.convert_tokens_to_string(tokens)


def verify_start_mark(seq, start_mark):
    """âœ… è‡ªåŠ¨æ ¡éªŒå‡½æ•°ï¼ˆæ ¡éªŒæ ‡å‡†ä¸å˜ï¼‰"""
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
    token_list = [tokenizer._convert_id_to_token(id) for id in seq]
    report = {"is_pass": True, "error_info": []}
    if len(seq) != len(start_mark):
        report["is_pass"] = False
        report["error_info"].append(f"é•¿åº¦ä¸ä¸€è‡´ï¼seq={len(seq)}, start_mark={len(start_mark)}")
        return report

    for idx in range(len(token_list)):
        token = token_list[idx]
        sm_val = start_mark[idx]
        if token in ["[CLS]", "[SEP]"] and sm_val != 0:
            report["is_pass"] = False
            report["error_info"].append(f"ä¸‹æ ‡{idx}ï¼š{token} â†’ start_mark={sm_val}ï¼ˆå¿…é¡»ä¸º0ï¼‰")
        elif token in PUNCTUATIONS and sm_val == 1:
            report["is_pass"] = False
            report["error_info"].append(f"ä¸‹æ ‡{idx}ï¼š{token}(æ ‡ç‚¹) â†’ start_mark=1ï¼ˆå¿…é¡»ä¸º0ï¼‰")
        elif not token.startswith("##") and token not in ["[CLS]",
                                                          "[SEP]"] and token not in PUNCTUATIONS and sm_val == 0:
            report["is_pass"] = False
            report["error_info"].append(f"ä¸‹æ ‡{idx}ï¼š{token}(æ–°è¯å¼€å¤´) â†’ start_mark=0ï¼ˆå¿…é¡»ä¸º1ï¼‰")
    return report


# ===================== ä¸»ç¨‹åºï¼ˆè¿è¡Œ+æ ¡éªŒä¸€ä½“åŒ–ï¼‰ =====================
if __name__ == '__main__':
    torch.manual_seed(args.seed)
    # 1. ç”Ÿæˆä¿®å¤åçš„æ•°æ®
    test_data = FraudAttackDataset(BASE_DATA_PATH)

    # 2. éšæœºæŠ½å–1æ¡æ ·æœ¬ï¼Œæ‰§è¡Œè‡ªåŠ¨æ ¡éªŒï¼ˆéªŒè¯ä¿®å¤æ•ˆæœï¼‰
    if len(test_data) > 0:
        sample = test_data[0]
        seq = sample['seq']
        start_mark = sample['start_mark']
        check_report = verify_start_mark(seq, start_mark)

        print("\n" + "=" * 80)
        print("âœ… ä¿®å¤å start_mark è‡ªåŠ¨æ ¡éªŒç»“æœ")
        print("=" * 80)
        if check_report["is_pass"]:
            print(Fore.GREEN + "ğŸ‰ æ ¡éªŒ100%é€šè¿‡ï¼æ— ä»»ä½•å¼‚å¸¸ï¼Œstart_markä¸è¯æ±‡ç²¾å‡†ä¸€ä¸€å¯¹åº”ï¼")
        else:
            print(Fore.RED + "âŒ ä»æœ‰å¼‚å¸¸ï¼š")
            for err in check_report["error_info"]:
                print(f"â†’ {err}")

        # æ‰“å°å…³é”®ä¿¡æ¯
        print(Fore.RESET + "\nğŸ“Œ æ ·æœ¬å…³é”®ä¿¡æ¯ï¼š")
        print(f"â†’ seqé•¿åº¦ï¼š{len(seq)}")
        print(f"â†’ start_marké•¿åº¦ï¼š{len(start_mark)}")
        print(f"â†’ knowledge_dictæœ‰æ•ˆè¯æ¡æ•°ï¼š{len(sample['knowledge_dict'])}")
        print(f"â†’ start_markæ ‡è®°ç¤ºä¾‹ï¼ˆå‰20ä½ï¼‰ï¼š{start_mark[:20]}")